ğŸ§  In LSTM, we have TWO types of memory

h_t â†’ Hidden state (short-term memory)
c_t â†’ Cell state (long-term memory)
Both are updated at every timestep.

â­ Is hâ‚™ the previous or recent memory?
h_n = the final hidden state after the last time step.
It represents:
short-term memory
what LSTM wants to â€œoutputâ€ at each step
So hâ‚™ = recent (latest) memory.

â­ How is câ‚™ calculated?
c_n = final cell state after the last time step
This is the long-term memory and is computed using the LSTM formula:
c_t = f_t * c_(t-1) + i_t * g_t
Where:
f_t = forget gate
i_t = input gate
g_t = candidate new memory
c_(t-1) = previous cell state
Letâ€™s break this into simple words:
âœ” Step 1: Forget old memory
forget_gate * previous_cell_state
âœ” Step 2: Add new important memory
input_gate * candidate_state
âœ” Step 3: Combine to form new cell state
c_t = (things to keep) + (things to add)
This updated c_t becomes the next timestepâ€™s c_(t-1).
At the final timestep:
c_n = last cell state (long-term memory)

â­ Relationship between hâ‚œ and câ‚œ
After computing c_t, the hidden state is calculated using the output gate:
h_t = output_gate * tanh(c_t)
So:
c_t = what LSTM stores
h_t = what LSTM outputs

ğŸ“Œ Simple Intuition
Memory Type.            Variable	            Meaning
Long-term memory	        câ‚™          	What the LSTM keeps for a long time
Short-term output memory	hâ‚™	            What the LSTM uses right now

âœ” Example in PyTorch Returning Values
If sequence length = 5, LSTM returns:
output â†’ hidden states at all 5 steps
h_n    â†’ hidden state at step 5 (final)
c_n    â†’ cell state at step 5 (final)
So yes:
hâ‚™ = recent short-term memory
câ‚™ = recent long-term memory
And c_n is ALWAYS calculated using:
forget gate + new info gate