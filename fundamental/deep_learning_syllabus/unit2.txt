Regularization :

Regularization is a technique used to stop a model from memorizing data and instead help it learn properly.
üëâ Think of it like this:
A good student understands concepts
A bad student memorizes answers
Regularization makes the model a good student üëç
It adds rules or limits so the model does not become too complex.

What is Overfitting?
Overfitting happens when a model:
Learns the training data too well
But fails on new (unseen) data
üìå The model remembers noise and small details that are not actually important.

Real-Life Example
Imagine preparing for an exam:
You memorize past question papers ‚Üí you score high on practice tests
In the real exam, questions change ‚Üí you fail
üëâ This is overfitting.

In Machine Learning Terms
Training accuracy ‚Üí very high
Testing accuracy ‚Üí low
This means the model has overfitted.

Why Overfitting is Bad
Model does not generalize
Performs poorly in real-world use
Gives false confidence

How Regularization Fixes Overfitting
Regularization controls the model‚Äôs freedom.
Common regularization methods (simple idea):
L1 / L2 Regularization ‚Üí stop weights from becoming too large or
L1 regularization ‚Üí makes weights sparse
L2 regularization ‚Üí reduces weight magnitude
Dropout ‚Üí randomly turn off neurons so model can‚Äôt depend on only a few
Early Stopping ‚Üí stop training before memorization starts
Data Augmentation ‚Üí show more varied data or increase dataset size

One-Line Exam Definitions
Overfitting: When a model performs well on training data but poorly on new data.
Regularization: Techniques used to reduce overfitting by controlling model complexity.

Batch-normalization:
Batch Normalization is a technique used to normalize layer activations for each mini-batch to reduce internal 
covariate shift. It normalizes the mean and variance of activations and then applies learnable scale and shift parameters. 
Batch Normalization improves training speed, stabilizes gradients, allows higher learning rates, and also acts as a 
regularizer.
or we can say that, Batch Normalization is a technique used in deep neural networks to normalize the activations of a
layer for each mini-batch, so that they have zero mean and unit variance, followed by learnable scaling and shifting.
üëâ Introduced by Ioffe and Szegedy (2015).

Why Batch Normalization is Needed
Deep networks suffer from:
(a) Internal Covariate Shift
Distribution of activations changes as network parameters update.
Slows training and causes instability.
(b) Problems Without BatchNorm
Slow convergence
Vanishing/exploding gradients
High sensitivity to learning rate
Overfitting
BatchNorm stabilizes training.

Placement of Batch Normalization in a Network
Correct Placement (Most Common)
Linear / Conv ‚Üí BatchNorm ‚Üí Activation (ReLU)

Why before activation?
Keeps normalized distribution before non-linearity
Improves gradient flow

Batch Normalization During Training vs Testing
During Training
  Mean and variance computed from mini-batch
  Running averages are maintained
During Testing (Inference)
  Use running mean and running variance
  No batch dependency ‚Üí stable predictions

Backpropagation in Batch Normalization
1.Gradients flow through:
 -normalization step
 -scaling (Œ≥)
 -shifting (Œ≤)
2.Œ≥ and Œ≤ are trained using gradient descent.
3.BatchNorm reduces vanishing gradient problem.
(üëâ You don‚Äôt need full derivation unless explicitly asked.)

Advantages of Batch Normalization
(Highly Important for Exams)
‚úÖ Faster convergence
‚úÖ Allows higher learning rates
‚úÖ Reduces vanishing/exploding gradients
‚úÖ Acts as regularizer
‚úÖ Less sensitive to initialization
‚úÖ Improves generalization

Disadvantages / Limitations
‚ùå Depends on batch size
‚ùå Performs poorly with very small batches
‚ùå Extra computation
‚ùå Less effective in RNNs (LayerNorm preferred)

Batch Normalization vs Other Normalization Techniques:
| Technique    | Normalization Scope | Used In            |
| ------------ | ------------------- | ------------------ |
| BatchNorm    | Across batch        | CNNs, DNNs         |
| LayerNorm    | Across features     | Transformers, RNNs |
| InstanceNorm | Per sample          | Style transfer     |
| GroupNorm    | Groups of channels  | Small batch CNNs   |

Batch Normalization in CNNs:
1.Applied per feature map
2.Mean and variance computed over:
 -batch size
 -spatial dimensions
This preserves spatial structure.

One-Line Exam Facts:
Introduced in 2015
Parameters: Œ≥ and Œ≤
Reduces internal covariate shift
Often removes need for dropout
Used heavily in CNNs

VC Dimension and Neural Nets (Exam Definition)
VC Dimension = a measure of model capacity (how many patterns it can "shatter").
Key points
Higher VC dimension ‚Üí more complex model
Deep networks have very high VC dimension
High VC ‚Üí more risk of overfitting unless properly regularized

Convolutional Neural Networks (CNNs):
CNNs are designed for image processing
Key layers
 Convolution layer ‚Üí feature extraction
 ReLU ‚Üí activation
 Pooling layer ‚Üí downsampling
 Fully connected layer ‚Üí classification
Advantages
 Fewer parameters (weight sharing)
 Automatically learns features
 Best for: Image classification, object detection, face recognition

1. What is a CNN?
A Convolutional Neural Network (CNN) is a deep learning model mainly used for image and spatial data processing.
Unlike traditional neural networks, CNNs exploit spatial structure using convolution and pooling operations.

2. Basic Architecture of CNN
A typical CNN consists of:
Input Layer
Convolution Layer
Activation Function (ReLU)
Pooling Layer (Max-Pooling / Average-Pooling)
Fully Connected Layer
Output Layer (Softmax / Sigmoid)

3. Convolution Layer (Brief)
Applies filters (kernels) over the image.
Extracts features like:
edges
corners
textures
shapes
Mathematical idea:Feature Map=Input‚äóFilter+Bias

‚≠ê 4. Pooling Layer (IMPORTANT)
What is Pooling?
Pooling is a down-sampling technique that reduces the spatial size of feature maps.
üëâ It is applied after convolution + activation.
Why Pooling is Used?
Reduces computation
Reduces number of parameters
Prevents overfitting
Makes model translation invariant
Retains important features
Types of Pooling
Max Pooling (Most common)
Average Pooling
Global Average Pooling

‚≠ê Max Pooling (Most Important for Exams)
Definition
Max pooling selects the maximum value from each region of the feature map.
Working of Max Pooling
Choose pool size (e.g., 2√ó2)
Slide window across feature map
Select max value from each region
Output size becomes smaller
Advantages of Max Pooling
Preserves strongest features
Reduces noise
Faster computation
Works well with ReLU
Exam Points (Write This!)
Max pooling reduces the spatial dimension while retaining dominant features, thus improving efficiency and robustness of CNNs.

‚≠ê 5. Fully Connected Layer (VERY IMPORTANT)
What is Fully Connected (FC) Layer?
In a Fully Connected layer:
Every neuron is connected to all neurons in the previous layer.
It behaves like a traditional neural network layer.
Purpose of FC Layer
Combines all extracted features
Performs final classification
Learns non-linear combinations of features
Working of FC Layer
Feature maps are flattened into a vector
Weighted sum is computed
Activation function applied
Output passed to final layer
Mathematical form 
        y=Wx+b
Activation in FC Layer
ReLU ‚Üí hidden FC layers
Softmax ‚Üí multi-class classification
Sigmoid ‚Üí binary classification
Why FC Layers are Placed at the End?
Earlier layers ‚Üí feature extraction
FC layers ‚Üí decision making
Disadvantage
Large number of parameters
Risk of overfitting
‚û° often combined with Dropout

‚≠ê 6. CNN Flow Summary (Write in Exams)
Input Image ‚Üí Convolution ‚Üí ReLU ‚Üí Pooling ‚Üí Convolution ‚Üí ReLU ‚Üí Pooling ‚Üí Flatten ‚Üí Fully Connected ‚Üí Output

‚≠ê 7. Pooling vs Fully Connected Layer (Comparison)
| Pooling Layer        | Fully Connected Layer   |
| -------------------- | ----------------------- |
| Reduces spatial size | Performs classification |
| No learnable weights | Has learnable weights   |
| Prevents overfitting | Prone to overfitting    |
| Feature selection    | Decision making         |

‚≠ê 8. Applications of CNN
Image classification
Face recognition
Medical image analysis
Object detection
Autonomous vehicles

Hidden Fully Connected (FC) layers: ReLU
Output FC layer:
Softmax ‚Üí multi-class classification
Sigmoid ‚Üí binary classification

Generative Adversarial Networks (GANs)
GAN = Generator + Discriminator
Generator (G)
Creates fake data.
Discriminator (D)
Tries to detect fake vs real data.
Training = Minimax game
             ùê∫:tries to fool ùê∑
             D:tries to detect fake
GANs create:
 Fake images
 Art
 Deepfakes
 Image-to-image transformation

Semi-Supervised Learning:
Uses few labeled + many unlabeled examples.
Methods
 Self-training ‚Üí model labels unlabeled data
 Consistency Regularization ‚Üí same input + noise = same output
 GAN-based semi-supervised ‚Üí discriminator also classifies labels
Useful when:
 Labels are expensive (medical images, legal data, etc.)

Semi-Supervised Learning is a learning approach where a small labeled dataset and a large unlabeled dataset are used 
together to train a model. It reduces labeling cost and improves model accuracy. Common techniques include self-training, 
co-training, graph-based methods, and consistency regularization. It is widely used in applications like medical imaging 
and speech recognition.