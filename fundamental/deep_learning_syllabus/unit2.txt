Regularization :

Regularization is a technique used to stop a model from memorizing data and instead help it learn properly.
ğŸ‘‰ Think of it like this:
A good student understands concepts
A bad student memorizes answers
Regularization makes the model a good student ğŸ‘
It adds rules or limits so the model does not become too complex.

What is Overfitting?
Overfitting happens when a model:
Learns the training data too well
But fails on new (unseen) data
ğŸ“Œ The model remembers noise and small details that are not actually important.

Real-Life Example
Imagine preparing for an exam:
You memorize past question papers â†’ you score high on practice tests
In the real exam, questions change â†’ you fail
ğŸ‘‰ This is overfitting.

In Machine Learning Terms
Training accuracy â†’ very high
Testing accuracy â†’ low
This means the model has overfitted.

Why Overfitting is Bad
Model does not generalize
Performs poorly in real-world use
Gives false confidence

How Regularization Fixes Overfitting
Regularization controls the modelâ€™s freedom.
Common regularization methods (simple idea):
L1 / L2 Regularization â†’ stop weights from becoming too large or
L1 regularization â†’ makes weights sparse
L2 regularization â†’ reduces weight magnitude
Dropout â†’ randomly turn off neurons so model canâ€™t depend on only a few
Early Stopping â†’ stop training before memorization starts
Data Augmentation â†’ show more varied data or increase dataset size

One-Line Exam Definitions
Overfitting: When a model performs well on training data but poorly on new data.
Regularization: Techniques used to reduce overfitting by controlling model complexity.

Batch-normalization:
Batch Normalization is a technique used to normalize layer activations for each mini-batch to reduce internal 
covariate shift. It normalizes the mean and variance of activations and then applies learnable scale and shift parameters. 
Batch Normalization improves training speed, stabilizes gradients, allows higher learning rates, and also acts as a 
regularizer.
or we can say that, Batch Normalization is a technique used in deep neural networks to normalize the activations of a
layer for each mini-batch, so that they have zero mean and unit variance, followed by learnable scaling and shifting.
ğŸ‘‰ Introduced by Ioffe and Szegedy (2015).

Why Batch Normalization is Needed
Deep networks suffer from:
(a) Internal Covariate Shift
Distribution of activations changes as network parameters update.
Slows training and causes instability.
(b) Problems Without BatchNorm
Slow convergence
Vanishing/exploding gradients
High sensitivity to learning rate
Overfitting
BatchNorm stabilizes training.

Placement of Batch Normalization in a Network
Correct Placement (Most Common)
Linear / Conv â†’ BatchNorm â†’ Activation (ReLU)

Why before activation?
Keeps normalized distribution before non-linearity
Improves gradient flow

Batch Normalization During Training vs Testing
During Training
  Mean and variance computed from mini-batch
  Running averages are maintained
During Testing (Inference)
  Use running mean and running variance
  No batch dependency â†’ stable predictions

Backpropagation in Batch Normalization
1.Gradients flow through:
 -normalization step
 -scaling (Î³)
 -shifting (Î²)
2.Î³ and Î² are trained using gradient descent.
3.BatchNorm reduces vanishing gradient problem.
(ğŸ‘‰ You donâ€™t need full derivation unless explicitly asked.)

Advantages of Batch Normalization
(Highly Important for Exams)
âœ… Faster convergence
âœ… Allows higher learning rates
âœ… Reduces vanishing/exploding gradients
âœ… Acts as regularizer
âœ… Less sensitive to initialization
âœ… Improves generalization

Disadvantages / Limitations
âŒ Depends on batch size
âŒ Performs poorly with very small batches
âŒ Extra computation
âŒ Less effective in RNNs (LayerNorm preferred)

Batch Normalization vs Other Normalization Techniques:
| Technique    | Normalization Scope | Used In            |
| ------------ | ------------------- | ------------------ |
| BatchNorm    | Across batch        | CNNs, DNNs         |
| LayerNorm    | Across features     | Transformers, RNNs |
| InstanceNorm | Per sample          | Style transfer     |
| GroupNorm    | Groups of channels  | Small batch CNNs   |

Batch Normalization in CNNs:
1.Applied per feature map
2.Mean and variance computed over:
 -batch size
 -spatial dimensions
This preserves spatial structure.

One-Line Exam Facts:
Introduced in 2015
Parameters: Î³ and Î²
Reduces internal covariate shift
Often removes need for dropout
Used heavily in CNNs

VC Dimension and Neural Nets (Exam Definition)
VC Dimension = a measure of model capacity (how many patterns it can "shatter").
Key points
Higher VC dimension â†’ more complex model
Deep networks have very high VC dimension
High VC â†’ more risk of overfitting unless properly regularized

Convolutional Neural Networks (CNNs):
CNNs are designed for image processing
Key layers
 Convolution layer â†’ feature extraction
 ReLU â†’ activation
 Pooling layer â†’ downsampling
 Fully connected layer â†’ classification
Advantages
 Fewer parameters (weight sharing)
 Automatically learns features
 Best for: Image classification, object detection, face recognition

Generative Adversarial Networks (GANs)
GAN = Generator + Discriminator
Generator (G)
Creates fake data.
Discriminator (D)
Tries to detect fake vs real data.
Training = Minimax game
             ğº:tries to fool ğ·
             D:tries to detect fake
GANs create:
 Fake images
 Art
 Deepfakes
 Image-to-image transformation