UNIT â€“ 1 : INTRODUCTION (Deep Learning)

1. Introduction to Machine Learning
What is Machine Learning?
Machine Learning (ML) is a method where a computer learns patterns from data instead of being manually programmed.
Simple Example:
Give many images of cats â†’ ML learns the pattern â†’ Predicts â€œcatâ€ for new image.
Types of ML
Supervised Learning â€“ Labeled data (e.g., spam / not spam).
Unsupervised Learning â€“ No labels (e.g., clustering customers).
Reinforcement Learning â€“ Learning by trial & reward (like playing games).

2. Linear Models
Linear models predict using a line/hyperplane:
ð‘¦=ð‘¤^ð‘‡*ð‘¥+ð‘
Where â†’
w = weights
b = bias
x = input features

2.1 Logistic Regression
Used for binary classification (Yes/No).
Key Idea:
Logistic Regression outputs probability using sigmoid:
ðœŽ(ð‘§)=1/1+ð‘’^âˆ’ð‘§
Decision:
If probability > 0.5 â†’ Class 1
Else â†’ Class 0

2.2 Perceptron
The simplest neural network (1 neuron).
Working:
Take weighted sum:
ð‘§=ð‘¤^ð‘‡ð‘¥+ð‘
Apply step function:
If z > 0 â†’ 1
Else â†’ 0
Perceptron Limitation:
Cannot solve non-linear problems (like XOR).

2.3 Support Vector Machines (SVMs)
SVM finds the best separating line (hyperplane) between two classes.
Goal:
Maximize margin (distance between line and nearest points).
Diagram (simple):
Class 1: o o o
               \   <-- decision boundary
Class 2: x x x
Important Terms
Support Vectors â†’ Points closest to boundary
Kernel Trick â†’ Converts non-linear data into linear form

3. INTRO TO NEURAL NETWORKS
3.1 What a Shallow Neural Network Computes
A shallow network = 1 hidden layer.
Diagram
Input â†’ [Hidden Layer] â†’ Output
Mathematical Form:
Hidden layer:h=Ïƒ(W1â€‹x+b1â€‹)
Output layer:y=Ïƒ(W2â€‹h+b2â€‹)
Neural nets learn weights that turn inputs â†’ useful features â†’ predictions.

4. TRAINING A NETWORK
Training = finding best weights using loss functions + backpropagation + SGD.

4.1 Loss Functions
Tell how â€œwrongâ€ the model is.
Common Ones:
MSE (Mean Squared Error) â†’ Regression
Cross-Entropy Loss â†’ Classification

4.2 Backpropagation
Simple meaning:
Backprop tells each weight how much it contributed to the error and how to update it.
Steps (easy):
Do forward pass â†’ get prediction
Compare with actual â†’ compute loss
Send error backward â†’ compute gradients
Update weights

4.3 Stochastic Gradient Descent (SGD)
SGD updates weights using:w=wâˆ’Î·â‹…âˆ‚w/âˆ‚Lâ€‹
Where:
Î· (eta) = learning rate
L = loss
gradient = direction of increasing error
SGD uses one sample at a time, making it fast.

5. Neural Networks as Universal Function Approximators
The Universal Approximation Theorem
A neural network with one hidden layer can approximate ANY continuous function.
Meaning â†’
Given enough neurons, a neural network can learn almost any pattern (image â†’ label, audio â†’ text, etc.).

EXAM-ORIENTED SHORT NOTES
âœ” Perceptron vs Logistic Regression
Perceptron	        Logistic Regression
Hard output (0/1)	Probability output
Step function	    Sigmoid function
Not differentiable	Differentiable
Cannot solve XOR	Can solve XOR with feature mapping
