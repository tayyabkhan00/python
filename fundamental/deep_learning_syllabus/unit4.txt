OPTIMIZATION AND GENERALIZATION

1. What is Optimization? (1-line recap)
Optimization in deep learning means finding the best weights (parameters) that minimize the loss function.

1. Optimization in Deep Learning

Optimization = how a neural network learns best weights
Goal: Minimize loss function (error between predicted & actual output)
We update weights using gradients (from backpropagation)
Common Optimizers
Gradient Descent ‚Äì slow, uses full dataset
SGD (Stochastic Gradient Descent) ‚Äì fast, noisy
Momentum ‚Äì speeds up learning
Adam ‚Äì most popular (adaptive + momentum)
Exam Point (5 marks):
Optimization in deep learning is the process of adjusting model parameters to minimize the loss function using 
gradient-based algorithms like SGD and Adam.

2. Non-Convex Optimization for Deep Networks

Deep learning optimization is a non-convex problem because deep neural networks use multiple layers and non-linear 
activation functions, resulting in complex loss surfaces with many local minima and saddle points. This makes 
optimization challenging and prevents guarantees of global minima. However, stochastic gradient descent, random 
initialization, and over-parameterization enable models to escape saddle points and converge to sufficiently good solutions.

Deep learning loss surfaces are non-convex ‚ùå
That means:
Many local minima
Saddle points (flat regions)
No guarantee of global minimum

Why still works?
Large networks ‚Üí many good-enough minima
SGD noise helps escape saddle points

Key Challenges
Vanishing gradients
Exploding gradients
Slow convergence

Exam Tip (10 marks):
Deep networks involve non-convex optimization problems, making training difficult due to multiple local minima and 
saddle points.
Minima, saddle points, and flat regions are points on the loss surface, whereas the gradient is the slope of the 
loss function at those points that guides weight updates.

3. Stochastic Optimization

Stochastic optimization updates model parameters using randomly selected samples or mini-batches instead of the full dataset.

Instead of full data ‚Üí use mini-batches
Why?
Faster
Uses less memory
Adds randomness (helps generalization)

Types
SGD
Mini-batch SGD
Adam, RMSProp
Formula (not mandatory but useful):Œ∏=Œ∏‚àíŒ∑‚àáL(Œ∏)
Stochastic optimization updates parameters using a random subset of training data, improving speed and scalability.

‚úÖWhy Noise is GOOD in Stochastic Optimization?
Noise = randomness in gradient updates.
Benefits:
Helps escape local minima
Escapes saddle points
Leads to better generalization
üëâ Deterministic optimization may get stuck
üëâ Stochastic optimization explores better

Strong Exam Line (10/10):

Due to the non-convex nature of deep learning loss functions, stochastic optimization methods such as SGD and Adam are 
preferred as they provide efficient and scalable approximate solutions.

4. Generalization in Neural Networks

Generalization = performing well on unseen data

Problems
Overfitting ‚Üí memorizes training data
Underfitting ‚Üí model too simple

Techniques to Improve Generalization
Regularization (L1, L2)
Dropout
Early stopping
Data augmentation
Batch normalization

Exam Statement:
A model generalizes well if it performs equally well on training and test data.